# -*- coding: utf-8 -*-
"""CIBMTR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184ShSdtFuEKY0bz9rdr7Lv_qWZLO6DKp
"""

# !pip install lifelines

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from lifelines.utils import concordance_index

train = pd.read_csv('train.csv')
train

testdata = pd.read_csv('test.csv')
test_data = testdata
test_data.head()

# print(train.info())
# print(train.describe())

"""# Data pre-proccesing

#### Filling Null values with Median for numerical Feature
"""

num_cols = train.select_dtypes(include=['float64', 'int64']).columns
num_cols_test = test_data.select_dtypes(include=['float64', 'int64']).columns

print(train[num_cols].head())
imputer = SimpleImputer(strategy='median')
train[num_cols] = imputer.fit_transform(train[num_cols])

imputer_test = SimpleImputer(strategy='median')
test_data[num_cols_test] = imputer_test.fit_transform(test_data[num_cols_test])
test_data[num_cols_test] = imputer_test.transform(test_data[num_cols_test])

"""#### Impute missing categorical values with the most frequent category

"""

cat_cols = train.select_dtypes(include=['object']).columns
print(train[cat_cols].head())
imputer = SimpleImputer(strategy='most_frequent')
train[cat_cols] = imputer.fit_transform(train[cat_cols])

cat_cols_test = test_data.select_dtypes(include=['object']).columns
imputer_test_cat = SimpleImputer(strategy='most_frequent')
test_data[cat_cols_test] = imputer_test_cat.fit_transform(test_data[cat_cols_test])
test_data[cat_cols_test] = imputer_test_cat.transform(test_data[cat_cols_test])

"""#### Label Encoding (for ordinal variables)"""

print(train['race_group'].head())

label_encoder = LabelEncoder()
train['race_group'] = label_encoder.fit_transform(train['race_group'])
test_data['race_group'] = label_encoder.transform(test_data['race_group'])
print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

"""#### One-Hot Encoding (for nominal variables)

"""

train = pd.get_dummies(train, columns=['sex_match', 'graft_type', 'cmv_status'], drop_first=True)
test_data = pd.get_dummies(test_data, columns=['sex_match', 'graft_type', 'cmv_status'], drop_first=True)

"""### Split Data

"""

X = train.drop(columns=['efs', 'efs_time'])
y = train[['efs', 'efs_time']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### Preprocess using ColumnTransformer

"""

categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns

categorical_cols_test = test_data.select_dtypes(include=['object']).columns
numerical_cols_test = test_data.select_dtypes(include=['float64', 'int64']).columns

preprocessor_test = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols_test),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols_test)
    ]
)

from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

X_train = preprocessor.fit_transform(X_train)
X_test = preprocessor.transform(X_test)
test_data = preprocessor_test.fit_transform(test_data)
print(len(test_data))



"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train['efs'])

y_pred_lr = log_reg.predict(X_test)

accuracy = accuracy_score(y_test['efs'], y_pred_lr)
f1 = f1_score(y_test['efs'], y_pred_lr)
f1

"""## Decision Tree"""

from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(random_state=42, )
dt_model.fit(X_train, y_train['efs_time'])

y_pred_dt = dt_model.predict(X_test)

c_index_dt = concordance_index(y_test['efs_time'], y_pred_dt)
c_index_dt

"""## Random Forest"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=120, random_state=42)
model.fit(X_train, y_train['efs_time'])

y_pred = model.predict(X_test)

c_index = concordance_index(y_test['efs_time'], y_pred)
c_index

pred_rand_for = model.predict(test_data)

"""## Gradient Boosting with XGBoost


"""

from xgboost import XGBRegressor

!pip install scikit-learn==1.5.2

y_train_efs_time = y_train['efs_time'].to_numpy()

xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.01, random_state=42)
xgb_model.fit(X_train, y_train['efs_time'])



"""## Gradient Boosting with LightGBM"""

from lightgbm import LGBMRegressor

lgbm_model = LGBMRegressor(n_estimators=150, learning_rate=0.05, random_state=42)
lgbm_model.fit(X_train, y_train['efs_time'])

y_pred_lgbm = lgbm_model.predict(X_test)

c_index_lgbm = concordance_index(y_test['efs_time'], y_pred_lgbm)
c_index_lgbm

pred =

"""## Gradient Boosting with CatBoost"""

!pip install catboost
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, random_seed=42, verbose=0)
cat_model.fit(X_train, y_train['efs_time'])

y_pred_cat = cat_model.predict(X_test)

c_index_cat = concordance_index(y_test['efs_time'], y_pred_cat)
c_index_cat

